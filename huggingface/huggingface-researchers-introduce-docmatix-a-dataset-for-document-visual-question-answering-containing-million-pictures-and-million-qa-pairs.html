<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>HuggingFace Researchers Introduce Docmatix: A Dataset For Document Visual Question Answering Containing 2.4 Million Pictures And 9.5 Million Q/A Pairs | The Freedom  Project</title>

	<meta name="description" content="">

	<!-- Open Graph meta tags (for Facebook, LinkedIn, etc.) -->
	<meta property="og:title" content="HuggingFace Researchers Introduce Docmatix: A Dataset For Document Visual Question Answering Containing 2.4 Million Pictures And 9.5 Million Q/A Pairs">
	<meta property="og:description" content="">
	<meta property="og:image" content="https://www.marktechpost.com/wp-content/uploads/2024/07/Screenshot-2024-07-23-at-1.01.00-AM.png">
	<meta property="og:url" content="https://www.marktechpost.com/2024/07/23/huggingface-researchers-introduce-docmatix-a-dataset-for-document-visual-question-answering-containing-2-4-million-pictures-and-9-5-million-q-a-pairs/">

	<!-- Twitter Card meta tags (for Twitter) -->
	<meta name="twitter:card" content="https://www.marktechpost.com/wp-content/uploads/2024/07/Screenshot-2024-07-23-at-1.01.00-AM.png">
	<meta name="twitter:title" content="HuggingFace Researchers Introduce Docmatix: A Dataset For Document Visual Question Answering Containing 2.4 Million Pictures And 9.5 Million Q/A Pairs">
	<meta name="twitter:description" content="">
	<meta name="twitter:image" content="https://www.marktechpost.com/wp-content/uploads/2024/07/Screenshot-2024-07-23-at-1.01.00-AM.png">
	
	<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">

	<!-- Include Font Awesome CSS -->
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">

	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">


	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Poppins:ital,wght@0,400;0,500;1,400&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Condensed&display=swap" rel="stylesheet">

	<link href="https://fonts.googleapis.com/css2?family=Alegreya+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;0,800;0,900;1,100;1,300;1,400;1,500;1,700;1,800;1,900&display=swap" rel="stylesheet">

	<style>
		.alegreya-sans-thin {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 100;
		  font-style: normal;
		}

		.alegreya-sans-thin-italic {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 100;
		  font-style: italic;
		}

		.alegreya-sans-light {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 300;
		  font-style: normal;
		}

		.alegreya-sans-light-italic {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 300;
		  font-style: italic;
		}

		.alegreya-sans-regular {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 400;
		  font-style: normal;
		}

		.alegreya-sans-regular-italic {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 400;
		  font-style: italic;
		}

		.alegreya-sans-medium {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 500;
		  font-style: normal;
		}

		.alegreya-sans-medium-italic {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 500;
		  font-style: italic;
		}

		.alegreya-sans-bold {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 700;
		  font-style: normal;
		}

		.alegreya-sans-bold-italic {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 700;
		  font-style: italic;
		}

		.alegreya-sans-extrabold {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 800;
		  font-style: normal;
		}

		.alegreya-sans-extrabold-italic {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 800;
		  font-style: italic;
		}

		.alegreya-sans-black {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 900;
		  font-style: normal;
		}

		.alegreya-sans-black-italic {
		  font-family: "Alegreya Sans", sans-serif;
		  font-weight: 900;
		  font-style: italic;
		}


		.roboto-thin {
		  font-family: "Roboto", sans-serif;
		  font-weight: 100;
		  font-style: normal;
		}

		.roboto-light {
		  font-family: "Roboto", sans-serif;
		  font-weight: 300;
		  font-style: normal;
		}

		.roboto-regular {
		  font-family: "Roboto", sans-serif;
		  font-weight: 400;
		  font-style: normal;
		}

		.roboto-medium {
		  font-family: "Roboto", sans-serif;
		  font-weight: 500;
		  font-style: normal;
		}

		.roboto-bold {
		  font-family: "Roboto", sans-serif;
		  font-weight: 700;
		  font-style: normal;
		}

		.roboto-black {
		  font-family: "Roboto", sans-serif;
		  font-weight: 900;
		  font-style: normal;
		}

		.roboto-thin-italic {
		  font-family: "Roboto", sans-serif;
		  font-weight: 100;
		  font-style: italic;
		}

		.roboto-light-italic {
		  font-family: "Roboto", sans-serif;
		  font-weight: 300;
		  font-style: italic;
		}

		.roboto-regular-italic {
		  font-family: "Roboto", sans-serif;
		  font-weight: 400;
		  font-style: italic;
		}

		.roboto-medium-italic {
		  font-family: "Roboto", sans-serif;
		  font-weight: 500;
		  font-style: italic;
		}

		.roboto-bold-italic {
		  font-family: "Roboto", sans-serif;
		  font-weight: 700;
		  font-style: italic;
		}

		.roboto-black-italic {
		  font-family: "Roboto", sans-serif;
		  font-weight: 900;
		  font-style: italic;
		}

		.poppins-regular {
		  font-family: "Poppins", sans-serif;
		  font-weight: 400;
		  font-style: normal;
		}

		.poppins-medium {
		  font-family: "Poppins", sans-serif;
		  font-weight: 500;
		  font-style: normal;
		}

		.poppins-regular-italic {
		  font-family: "Poppins", sans-serif;
		  font-weight: 400;
		  font-style: italic;
		}

		.roboto-condensed-regular {
		  font-family: "Roboto Condensed", sans-serif;
		  font-optical-sizing: auto;
		  font-weight: 400;
		  font-style: normal;
		}

		h1, h2, h3, h4, h5{
		  font-family: "Roboto Condensed", sans-serif;
		  font-optical-sizing: auto;
		  font-weight: 400;
		  font-style: normal;		}
		}

		p{
		  font-family: "Poppins", sans-serif;
		  font-weight: 400;
		  font-style: normal;
		}
	</style>


    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PPP5N1WD9F"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-PPP5N1WD9F');
    </script>

</head>
<body>
	<section>
		<!-- As a link -->
		<nav class="navbar navbar-dark bg-dark fixed-top">
		  <div class="container-fluid">
		    <a class="navbar-brand roboto-condensed-regular" href="">The Freedom Project</a>
		  </div>
		</nav>
	</section>

	<section>
		<div class="container">
			<div class="row mt-5">
				<div class="col-12 mt-5 d-flex justify-content-center align-items-center flex-column" id="title-container">
					<h3 class="alegreya-sans-medium text-center">HuggingFace Researchers Introduce Docmatix: A Dataset For Document Visual Question Answering Containing 2.4 Million Pictures And 9.5 Million Q/A Pairs</h3>
					<!--<p class="roboto-medium text-secondary mb-0">Authors</p>-->
					<!--
						<p class="alegreya-sans-regular mt-0 text-secondary mb-0">By Dhanshree Shripad Shenwai, www.facebook.com</p>
					--->
					<p class="alegreya-sans-regular mt-0 text-secondary">23 July, 2024</p>

				</div>



				<div class="col-12 mt-2 mb-5 alegreya-sans-light d-flex justify-content-center" id="image-container">
					<img src="https://www.marktechpost.com/wp-content/uploads/2024/07/Screenshot-2024-07-23-at-1.01.00-AM.png" class="img-fluid shadow" style="max-height: 280px; object-fit: cover; border-radius: 10px; border: none">
				</div>

				<div class="col-12 mb-3 alegreya-sans-regular" id="secondary-keyword-summary" style="font-size: 18px">
					<div>
             
<p>Document Visual Question Answering (DocVQA) is a branch of visual question answering that focuses on answering queries about the contents of documents. These documents can take several forms, including scanned photographs, PDFs, and digital documents with text and visual features. However, there are few datasets for DocVQA because collecting and annotating the data is complicated. It requires understanding the context, structure, and layout of various document formats, which requires much manual effort. Due to the sensitive nature of the information contained within, many documents are inaccessible or have privacy concerns that make sharing or using them difficult. Domain-specific differences and the absence of document-structure uniformity further complicate the development of an exhaustive dataset. Factors contributing to the complexity of multi-modal fusion and the accuracy of optical character recognition also play a role.&#160;</p>



<p>Despite these challenges, the urgent need for more DocVQA datasets is underscored. These datasets are crucial for enhancing model performance, as they enable more thorough benchmarking and enhance model training for higher generalizability. By automating document-related processes across sectors and making documents more accessible through summary generation and query responding, updated DocVQA models could significantly impact document accessibility.</p> 





<p>To fine-tune Vision-Language Models (VLMs), and Idefics2 in particular, researchers from HuggingFace initially built The Cauldron, a massive collection of fifty datasets. As a result of these efforts, the team discovered a severe shortage of high-quality datasets for Document Visual Question Answering (DocVQA). With 10,000 photos and 39,000 question-answer (Q/A) pairings, DocVQA was the main dataset used for Idefics2. There still needs to be a significant performance disparity between open-source and closed-source models, even after fine-tuning this and other datasets.</p>



<p>Their new study introduces Docmatix, a monumental DocVQA dataset containing 2.4 million pictures and 9.5 million Q/A pairs extracted from 1.3 million PDF documents. This scale, which has increased by 240 times compared to earlier datasets, showcases the potential impact of Docmatix.</p> 





<p>The PDFA collection, which includes over two million PDFs, is the source of Docmatix. The researchers used a Phi-3-small model to create Q/A pairs using the PDFA transcriptions. To make sure the dataset was good, 15% of the Q/A pairings were removed that were found to be hallucinations during the creation filter. This was accomplished by eliminating responses that included the word &#8220;unanswerable&#8221; using regular expressions that detect code. There is a row in the dataset for every PDF. After processing the PDFs, the team saved 150 dpi photographs to the Hugging Face Hub. Now, anyone may access them with ease.&#160;</p>



<p>Users can place their full trust in Docmatix, as all PDFs can be traced back to the original PDFA dataset. Despite the resource-intensive process of converting several PDFs to photos, the researchers have uploaded the processed images for user convenience.</p>



<p>After processing the initial small dataset batch, the researchers ran multiple ablation experiments to fine-tune the prompts. They were aiming for approximately four question-and-answer pairs per page. A few pairs lack detail, whereas excess pairs indicate high overlap. Furthermore, they strived for responses resembling human speech, meaning they were neither lengthy nor brief. To avoid duplicating efforts, the questions were diverse. Surprisingly, there were few instances of question repetition when the Phi-3 model was instructed to inquire about certain details in the text (for example, &#8220;What are the titles of John Doe?&#8221;).</p>



<p>The team used the Florence-2 model to undertake ablation trials to assess Docmatix&#8217;s performance. To facilitate comparability, they trained a pair of versions of the model. The DocVQA dataset was used to train the initial version over multiple epochs. To ensure the model produced the right format for DocVQA evaluation, the second version was trained for one epoch on Docmatix (20% of the images and 4% of the Q/A pairs) and then for one epoch on DocVQA. The findings are noteworthy: a relative improvement of about 20% was produced by training on this tiny subset of Docmatix. Although much bigger, the 0.7B Florence-2 model only did 5% worse than the 8B Idefics2 model trained on various datasets.</p>



<p>The team hopes their work reduces the disparity between proprietary and open-sourced VLMs. To train a brand new, fantastic DocVQA model, they urge the open-source community to use Docmatix.&#160;</p>



 



<p>Check out the <strong><a href="https://huggingface.co/datasets/HuggingFaceM4/Docmatix" target="_blank" rel="noreferrer noopener">Dataset</a></strong> and <strong><a href="https://huggingface.co/blog/docmatix" target="_blank" rel="noreferrer noopener">Details</a></strong>. All credit for this research goes to the researchers of this project. Also,&#160;don&#8217;t forget to follow us on&#160;<strong><a href="https://twitter.com/Marktechpost">Twitter</a></strong> and join our&#160;<strong><a href="https://pxl.to/at72b5j" target="_blank" rel="noreferrer noopener">Telegram Channel</a></strong> and&#160;<a href="https://www.linkedin.com/groups/13668564/"><strong>LinkedIn Gr</strong></a><a href="https://www.linkedin.com/groups/13668564/" target="_blank" rel="noreferrer noopener"><strong>oup</strong></a>. <strong>If you like our work, you will love our</strong><a href="https://marktechpost-newsletter.beehiiv.com/subscribe" target="_blank" rel="noreferrer noopener"><strong>&#160;newsletter..</strong></a></p>



<p>Don&#8217;t Forget to join our&#160;<strong><a href="https://www.reddit.com/r/machinelearningnews/" target="_blank" rel="noreferrer noopener">46k+ ML SubReddit</a></strong></p>



 

 
 

   
         


        </div>
				</div>

				<!--<div class="col-12 mb-3 alegreya-sans-light d-flex justify-content-start" id="secondary-keyword-summary">
					<a class="btn btn-dark btn-lg shadow" href='https://www.marktechpost.com/2024/07/23/huggingface-researchers-introduce-docmatix-a-dataset-for-document-visual-question-answering-containing-2-4-million-pictures-and-9-5-million-q-a-pairs/'><div class="d-flex"><p class="me-2 mb-0">Continue Reading</p> <i class="bi bi-arrow-right-circle"></i></div></a>
				</div>-->


				<!--<div class="col-12 mb-4 roboto-condensed-regular mt-4">
					
				</div>-->
			</div>

		</div>
	</section>

	<section id="recentArticlesSection">

	</section>

	<section>
		<div class="row">
			<div class="col-12">
				<div class="d-flex justify-content-center py-1">
						<p class="alegreya-sans-medium text-center ms-2" style="font-size: 18px; color: grey;">The Freedom Project - &copy; <span id="currentYear"></span> ðŸ˜Œ </p>
				</div>
			</div>
		</div>
	</section>

	   <script>
        // Get the current year
        const currentYear = new Date().getFullYear();
        
        // Find the span element with the id 'currentYear'
        const yearElement = document.getElementById('currentYear');
        
        // Set the text content of the span element to the current year
        yearElement.textContent = currentYear;
    </script>

	<!-- Bootstrap Bundle from CDN -->
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>

	<!-- jQuery from CDN -->
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.1/jquery.min.js"></script>

	<script type="text/javascript">
		$(function(){
				$.getJSON("https://raw.githubusercontent.com/pdmarc7/thefreedomproject/main/data.json", function(data){
						//console.log(data)

						var randomSelectedArticles = getRandomElements(data, 6)

					$('#recentArticlesSection').append(
						$('<div>').addClass('container').append(
							$('<div>').addClass('row').append(
								$('<div>').addClass('col-12').append(
									$('<p>').addClass('h3 alegreya-sans-medium mb-2').text('Recent international news')
								),

								buildRandomArticleArr(randomSelectedArticles)
							)
						)
					)
				});

				function getRandomElements(arr, n) {
				    // Shuffle array using Fisher-Yates shuffle algorithm
				    for (let i = arr.length - 1; i > 0; i--) {
				        const j = Math.floor(Math.random() * (i + 1));
				        [arr[i], arr[j]] = [arr[j], arr[i]];
				    }

				    // Return the first n elements
				    return arr.slice(0, n);
				}



				function buildArticleCard(article){
					return $('<div>').addClass('card').append(

						$('<img>').attr({
							'src': article["top_image"],
							'alt': ''
						}).addClass('card-img-top'),

						$('<div>').addClass('card-body').append(
							$('<p>').addClass('h5 card-title alegreya-sans-medium').append(`${article.title}`),
							$('<a>').attr({
								'href': `http://${window.location.hostname}:${window.location.port}`+'/'+article.url
							}).addClass("btn btn-dark").append('Read More')
						)
					)
				}

				function buildRandomArticleArr(selectedArticles){
					var articles = []

					for (const n of selectedArticles){
						articles.push(
							$('<div>').addClass('col-12 col-md-4 mb-4').append(
								buildArticleCard(n)
							)
						)
					}

					return articles
				}

		})
	</script>


	<!-- GSAP and ScrollTrigger from CDN -->
	<!--<script src="https://cdn.jsdelivr.net/npm/gsap@3.9.1/dist/gsap.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/gsap@3.9.1/dist/ScrollTrigger.min.js"></script>-->
    <script>

    </script>

</body>
</html>